{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.12.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 해당 실습에 사용되는 데이터 셋은 트레이닝 데이터 600개, 테스트 데이터 100개로 구성\n",
    "\n",
    "- Batch size\n",
    "    * mini batch 크기 (트레이닝 데이터를 쪼개는 단위)\n",
    "    * mini batch 하나 당 파라미터 한번 업데이트\n",
    "    * 트레이닝 데이터를 통째로 신경망에 넣으면 비효율적 리소스 사용으로 학습시간 증가\n",
    "    * ==> 트레이닝 셋 작게 나눈다\n",
    "- Epoch\n",
    "    * 전체 트레이닝 셋이 신경망을 통과한 횟수\n",
    "    * ex) 1 epoch = Forward, Backward propagation 이 신경망을 통해 한번 진행됨\n",
    "- Iteration\n",
    "    * 1 epoch 를 마치는데 필요한 미니 배치 개수 (1 epoch를 마치는데 필요한 파라미터 업데이트 횟수)\n",
    "    * ex) 트레이닝 데이터 = 600개, Batch size = 10 ==> Iteration = 60 (1 epoch 시 paramater 60번 업데이트)\n",
    "\n",
    "- Learning rate (0~1 사이 값)\n",
    "    * loss function을 토대로 계산한 gradient 값에 learning rate 곱해서 원래 가중치에서 뺀다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_9 (Conv2D)           (None, 57, 57, 4)         772       \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 28, 28, 4)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 25, 25, 8)         520       \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 12, 12, 8)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 1152)              0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               295168    \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 299,030\n",
      "Trainable params: 299,030\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "145/145 [==============================] - 14s 90ms/step - loss: 1.4075 - accuracy: 0.5147\n",
      "Epoch 2/20\n",
      "145/145 [==============================] - 14s 94ms/step - loss: 0.6866 - accuracy: 0.7865\n",
      "Epoch 3/20\n",
      "145/145 [==============================] - 16s 107ms/step - loss: 0.4598 - accuracy: 0.8624\n",
      "Epoch 4/20\n",
      "145/145 [==============================] - 16s 111ms/step - loss: 0.3369 - accuracy: 0.8999\n",
      "Epoch 5/20\n",
      "145/145 [==============================] - 16s 108ms/step - loss: 0.2438 - accuracy: 0.9309\n",
      "Epoch 6/20\n",
      "145/145 [==============================] - 16s 109ms/step - loss: 0.1907 - accuracy: 0.9468\n",
      "Epoch 7/20\n",
      "145/145 [==============================] - 17s 115ms/step - loss: 0.1477 - accuracy: 0.9594\n",
      "Epoch 8/20\n",
      "145/145 [==============================] - 16s 111ms/step - loss: 0.1188 - accuracy: 0.9676\n",
      "Epoch 9/20\n",
      "145/145 [==============================] - 16s 111ms/step - loss: 0.0913 - accuracy: 0.9772\n",
      "Epoch 10/20\n",
      "145/145 [==============================] - 16s 110ms/step - loss: 0.0736 - accuracy: 0.9818\n",
      "Epoch 11/20\n",
      "145/145 [==============================] - 17s 114ms/step - loss: 0.0688 - accuracy: 0.9827\n",
      "Epoch 12/20\n",
      "145/145 [==============================] - 17s 114ms/step - loss: 0.0417 - accuracy: 0.9917\n",
      "Epoch 13/20\n",
      "145/145 [==============================] - 18s 127ms/step - loss: 0.0348 - accuracy: 0.9928\n",
      "Epoch 14/20\n",
      "145/145 [==============================] - 20s 135ms/step - loss: 0.0263 - accuracy: 0.9954\n",
      "Epoch 15/20\n",
      "145/145 [==============================] - 21s 148ms/step - loss: 0.0166 - accuracy: 0.9975\n",
      "Epoch 16/20\n",
      "145/145 [==============================] - 22s 152ms/step - loss: 0.0239 - accuracy: 0.9949\n",
      "Epoch 17/20\n",
      "145/145 [==============================] - 21s 143ms/step - loss: 0.0139 - accuracy: 0.9983\n",
      "Epoch 18/20\n",
      "145/145 [==============================] - 21s 141ms/step - loss: 0.0079 - accuracy: 0.9993\n",
      "Epoch 19/20\n",
      "145/145 [==============================] - 20s 141ms/step - loss: 0.0552 - accuracy: 0.9821\n",
      "Epoch 20/20\n",
      "145/145 [==============================] - 20s 141ms/step - loss: 0.0332 - accuracy: 0.9908\n",
      "Results for test data=\n",
      "0.9433656930923462\n",
      "78/78 [==============================] - 1s 10ms/step\n",
      "[[238   0   2   2   0   3   2   0   1   1]\n",
      " [  0 236   0   0   1   0   0   3   1   1]\n",
      " [  8   1 213   3   0   2   8   5   1   6]\n",
      " [  0   0   0 248   0   0   0   0   1   0]\n",
      " [  2   0   1   0 238   1   5   1   0   1]\n",
      " [ 10   0   2   3   0 233   2   0   0   1]\n",
      " [  7   0   1   1   0   1 239   0   0   0]\n",
      " [  0   3   2   0   0   0   1 232   0   0]\n",
      " [ 14   4   0   0   0   2   6   0 222   1]\n",
      " [  1   4   2   0   2   2   2   1   2 233]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import cv2\n",
    "\n",
    "tr_file = \"./list_tr1\"\n",
    "ts_file = \"./list_ts1\"\n",
    "n_epochs = int(input(\"Enter number of epochs : \"))\n",
    "b_size = int(input(\"Enter batch size : \"))      \n",
    "n_tag = 10\n",
    "l_rate = float(input(\"Enter learning rate : \"))\n",
    "SX = SY = 64    # 데이터 이미지 크기 : 64 x 64\n",
    "\n",
    "def Getdata(list_file):\n",
    "    fp = open(list_file, 'r')               \n",
    "    lines = fp.read().splitlines()      # line 단위로 파일 읽는다        \n",
    "    inp = []\n",
    "    tag = []\n",
    "    \n",
    "    for line in lines:\n",
    "        img_file, id = line.split(' ')  # 공백을 기준으로 split (img_file:파일 경로, id:target (class 정보))\n",
    "        #print(img_file, id)\n",
    "        img = cv2.imread(img_file)       \n",
    "        img = cv2.resize(img, dsize=(SY, SX)) # 64x64로 resize\n",
    "        inp.append(img/255)             # image data (0~255) 값을 0~1 값으로 Normalize\n",
    "        tag.append(np.uint8(id))        # String 2 integer\n",
    "        \n",
    "    fp.close()\n",
    "    return inp, tag     # list 반환\n",
    "\n",
    "# Training dataset 불러오기 (tr)\n",
    "[tr_inp, tr_tag] = Getdata(tr_file)\n",
    "tr_inp = np.array(tr_inp)\n",
    "tr_tag = np.array(tr_tag)\n",
    "\n",
    "# Test dataset 불러오기 (ts)\n",
    "[ts_inp, ts_tag] = Getdata(ts_file)\n",
    "ts_inp = np.array(ts_inp)\n",
    "ts_tag = np.array(ts_tag)\n",
    "\n",
    "#  [Layer 구성]  #\n",
    "model = models.Sequential()                                                         # layer 모델 초기화 (sequential : 연속적 레이어)\n",
    "model.add(layers.Conv2D(4,(8,8), activation='relu', input_shape=(64,64,3)))         # (11x11) 5개, 입력 데이터:64x64x3 (image라 channel 3개)\n",
    "model.add(layers.MaxPooling2D((2,2)))                                                   # 4x4 단위로 maxpooling\n",
    "model.add(layers.Conv2D(8,(4,4), activation='relu'))                                    # 2D conv (7x7) 7개 \n",
    "model.add(layers.MaxPooling2D((2,2)))  \n",
    "model.add(layers.Flatten())                                                             # 1차원으로 재배치\n",
    "model.add(layers.Dense(256, activation='relu'))                                         # MLP (Multi-layer perceptron), 노드 개수:128  \n",
    "model.add(layers.Dense(n_tag, activation='softmax'))                            # MLP, 최종 노드 개수는 class 개수랑 동일해야 = n_tag -> linear이면 Regression\n",
    "\n",
    "model.summary()  # 화면에 layer 표시\n",
    "\n",
    "#  [학습, 테스트]  #\n",
    "adam = optimizers.Adam(learning_rate = l_rate)  \n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])   # adam 알고리즘, loss 계산은 advanced crossentropy, 적중률\n",
    "model.fit(tr_inp, tr_tag, batch_size=b_size, epochs=n_epochs, verbose=1, shuffle=1)     # 학습 시작 (training dataset 입력, batch size, epoch 입력)\n",
    "test_loss, test_acc = model.evaluate(ts_inp, ts_tag, verbose=0)  # 모델 테스트 : loss, 정확도(적중률) 측정 -> loss는 낮을수록 좋고, acc는 높을수록 좋다\n",
    "print(\"Results for test data=\")\n",
    "print(test_acc)  # 테스트 결과(accuracy) 출력 \n",
    "\n",
    "model.save(\"model_7.h5\")\n",
    "\n",
    "#  [Confusion matrix 제공]  # \n",
    "#  테스트 결과 출력 : class별로 어떤걸로 판단했는지 횟수 제공  # \n",
    "pred = model.predict(ts_inp)\n",
    "y_pred = np.argmax(pred, axis=1)\n",
    "conf_mat = confusion_matrix(ts_tag, y_pred)\n",
    "print(conf_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 54, 54, 5)         1820      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 13, 13, 5)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 7, 7, 7)           1722      \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 1, 1, 7)          0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 7)                 0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               1024      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,856\n",
      "Trainable params: 5,856\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Results for test data=\n",
      "0.34554973244667053\n",
      "6/6 [==============================] - 0s 8ms/step\n",
      "[[ 3  1  2  1  0  3  5  0  2  3]\n",
      " [ 0  4  0  0  0  0  2  0  1  1]\n",
      " [ 0  1  3  1  1  0  1  0  1 11]\n",
      " [ 1  3  0 13  0  0  1  0  0  4]\n",
      " [ 1  1  1  0  0  2  3  2  5  7]\n",
      " [15  0  0  2  0 13  3  0  0  0]\n",
      " [ 6  0  0  0  0  1 14  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [10  0  1  4  0  2  6  0  0  0]\n",
      " [ 0  2  0  0  3  2  0  0  0 16]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import cv2\n",
    "\n",
    "SY = SX = 64\n",
    "\n",
    "def Getdata(list_file):\n",
    "    fp = open(list_file, 'r')               \n",
    "    lines = fp.read().splitlines()      # line 단위로 파일 읽는다        \n",
    "    inp = []\n",
    "    tag = []\n",
    "    \n",
    "    for line in lines:\n",
    "        img_file, id = line.split(' ')  # 공백을 기준으로 split (img_file:파일 경로, id:target (class 정보))\n",
    "        img = cv2.imread(img_file)   \n",
    "        img = cv2.resize(img, dsize=(SY, SX)) # 64x64로 resize\n",
    "        inp.append(img/255)             # image data (0~255) 값을 0~1 값으로 Normalize\n",
    "        tag.append(np.uint8(id))        # String 2 integer\n",
    "        \n",
    "    fp.close()\n",
    "    return inp, tag     # list 반환\n",
    "\n",
    "model = load_model(\"model_7.h5\")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "ts_file = \"./list_tr3\"\n",
    "[ts_inp2, ts_tag2] = Getdata(ts_file)\n",
    "ts_inp2 = np.array(ts_inp2)\n",
    "ts_tag2 = np.array(ts_tag2)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(ts_inp2, ts_tag2, verbose=0)  # 모델 테스트 : loss, 정확도(적중률) 측정 -> loss는 낮을수록 좋고, acc는 높을수록 좋다\n",
    "print(\"Results for test data=\")\n",
    "print(test_acc)  # 테스트 결과(accuracy) 출력 \n",
    "\n",
    "pred = model.predict(ts_inp2)\n",
    "y_pred = np.argmax(pred, axis=1)\n",
    "conf_mat = confusion_matrix(ts_tag2, y_pred)\n",
    "print(conf_mat)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch = 30, Batch size = 10, Learning rate = 0.01\n",
    "\n",
    " - Results for test data=\n",
    " - 0.8833333253860474\n",
    " - 10/10 [==============================] - 0s 9ms/step\n",
    " - [[38 12  0  0  0  0]\n",
    " -  [ 0 50  0  0  0  0]\n",
    " -  [ 0  0 36 14  0  0]\n",
    " -  [ 0  0  2 48  0  0]\n",
    " -  [ 0  0  0  0 43  7]\n",
    " -  [ 0  0  0  0  0 50]]\n",
    " \n",
    "\n",
    "---\n",
    "\n",
    "### Epoch = 50, Batch size = 15, Learning rate = 0.03\n",
    "- Results for test data=\n",
    "- 0.9900000095367432\n",
    "- 10/10 [==============================] - 0s 8ms/step\n",
    "- [[49  0  0  0  0  1]\n",
    "-  [ 0 50  0  0  0  0]\n",
    "-  [ 0  0 50  0  0  0]\n",
    "-  [ 0  0  2 48  0  0]\n",
    "-  [ 0  0  0  0 50  0]\n",
    "-  [ 0  0  0  0  0 50]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
